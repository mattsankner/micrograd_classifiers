{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa408e33-e181-44d3-87e7-2f89daa856fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc18d102-783d-477d-9814-6aa6f0ead931",
   "metadata": {},
   "source": [
    "# ```CIFAR-10``` Classifcation ! (This notebook is under construction...\n",
    "\n",
    "## ```CIFAR-10``` consists of $60,000, 32x32$ color images in $10$ different classes, with $6,000$ images per class. The dataset is divided into $50,000$ training images and $10,000$ test images. The classes don't overlap. We will classify these images using a version of ![micrograd](https://github.com/mattsankner/micrograd) neural network built in the repo at the link!\n",
    "\n",
    "### $10$ Classes in ```CIFAR-10```:\n",
    "- Airplane\n",
    "- Automobile\n",
    "- Bird\n",
    "- Cat\n",
    "- Deer\n",
    "- Dog\n",
    "- Frog\n",
    "- Horse\n",
    "- Ship\n",
    "- Truck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40f093c-f08b-4d31-9407-4a2a27fcd61f",
   "metadata": {},
   "source": [
    "### Loading/Preprocessing CIFAR-10 Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c10c715-aa19-4abf-bfb8-532367521f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 22:51:04.279476: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "710c16d6-6443-4161-89c3-42c7f1ebe68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fac3281-3638-4c83-a223-3d430db83a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the pixel values\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf2e52d1-b672-4629-9c8e-1559de094e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the input images\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50865d65-8de4-4fa7-b80c-471450887a5a",
   "metadata": {},
   "source": [
    "Below, we use ```one-hot encoding``` to convert categorical integer labels into binary vectors. ```to_categorical``` is a function used to convert integer labels into a one-hot encoded format.\n",
    "\n",
    "Example:\n",
    "Assume y_train contains the following class labels for $3$ images (instead of 10):\n",
    "\n",
    "- ```y_train``` = [0,1,2]\n",
    "\n",
    "Applying ```to_categorical(y_train,3)``` converts the labels into a one-hot encoded format:\n",
    "```python y_train =\n",
    "[\n",
    "[1,0,0]   #class 0\n",
    "[0,1,0]   #class 1\n",
    "[0,0,1]   #class 2\n",
    "]   \n",
    "```\n",
    "\n",
    "The only difference is we call ```to_categorical(y_train,10)``` !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56eda498-4639-4362-be7e-cb660096278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to categorical\n",
    "y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f619affc-b812-4fba-9250-f58a44d9a616",
   "metadata": {},
   "source": [
    "### Defining the Multi-Layer Perceptron Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dea0b49d-95b4-4526-9a57-db539447c3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the necessary imports for micrograd\n",
    "from micrograd.engine import Value\n",
    "from micrograd.mlp import Neuron, Layer, MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21fc4cd7-c3eb-46af-8d05-45236153ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a more complex MLP model for CIFAR-10\n",
    "class MLPComplex(MLP):\n",
    "    def __init__(self, nin, nouts):\n",
    "        super().__init__(nin, nouts)\n",
    "        self.dropout_rate = 0.2  # Adjusted dropout rate\n",
    "        print(f\"Initialized MLPComplex with {nin} inputs and {nouts} neurons in each layer\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "            if layer != self.layers[-1]:  # Don't apply ReLU activation and dropout to the output (last) layer\n",
    "                out = [o.relu() for o in out]\n",
    "                out = self.apply_dropout(out)\n",
    "        return out\n",
    "    \n",
    "    def apply_dropout(self, activations):\n",
    "        if self.dropout_rate > 0:\n",
    "            for i in range(len(activations)):\n",
    "                if random.random() < self.dropout_rate:\n",
    "                    activations[i] = Value(0.0)\n",
    "            print(f\"Applied dropout with rate {self.dropout_rate}, {sum(1 for a in activations if a.data == 0)} neurons dropped\")\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c66217e-efaf-454c-be64-ad1f8d2ad085",
   "metadata": {},
   "source": [
    "## MLP initialization:\n",
    "\n",
    "```python\n",
    "model = MLPComplex(3072, [1024, 512, 256, 10])\n",
    "```\n",
    "\n",
    "### Input Layer\n",
    "\n",
    "- $3072$: Each ```CIFAR-10``` image is $32x32$ pixels with $3$ color channels. Flattening this image results in a vector of size $32 x 32 x 3 = 3072$.  Therefore, the input layer has $3072$ neurons.\n",
    "  \n",
    "### Hidden Layers\n",
    "- $1024, 512, 256$: These are the numbers of neurons in the hidden layers. This was an empirical choice, but is a common way of doing it for ```DNN's```. Larger layers at the beginning generally help in capturing more complex patterns, and reducing size in subsequent layers helps in learning more abstract representations. \n",
    "\n",
    "### Output Layer\n",
    "- $10$: the output layer has $10$ neurons corresponding to the $10$ classes in ```CIFAR-10```. Each neuron represents the probability of the input image belonging to a particular class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6807756d-3b9e-4217-9f92-930564a8402b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized MLPComplex with 3072 inputs and [1024, 512, 256, 10] neurons in each layer\n",
      "Parameter count: 3805450\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = MLPComplex(3072, [1024, 512, 256, 10])  # CIFAR-10 images are 32x32x3 = 3072 pixels\n",
    "print(\"Parameter count:\", len(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c995e9c-0249-4c83-bb60-b66d0cf21f41",
   "metadata": {},
   "source": [
    "### Defining the Loss Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f6b0ec-fa30-4cbf-a3d6-140afa22fe9d",
   "metadata": {},
   "source": [
    "Here, as opposed to previous classification projects, we use ```categorical_cross_entropy()``` function instead of ```ReLU()``` to calculate the ```data_loss```. \n",
    "- ```target``` is this one-hot encoded vector representing the true class labels.\n",
    "- ```output``` is the vector of predicted probabilities output by the model for each class\n",
    "- ```target[i] * output[i].log()```: multiplies the target value (which is 1 for the correct class and 0 for others) by the logarithm of the predicted probability for that class.\n",
    "\n",
    "Note: we negate the sum to get the ```cross entropy loss```, which we aim to minimize.\n",
    "\n",
    "Example:\n",
    "\n",
    "``` python\n",
    "target = [0, 0, 1]\n",
    "output = [Value(0.2), Value(0.3), Value(0.5)]\n",
    "loss = -sum([target[i] * output[i].log() for i in range(len(target))])\n",
    "# loss = -(0*log(0.2) + 0*log(0.3) + 1*log(0.5))\n",
    "# loss = -log(0.5)\n",
    "```\n",
    "\n",
    "In the loss function, we then compute the cross entropy loss for each pair of the true label and predicted score in the batch. This results in a list of individual losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "809a9a05-084a-4df3-8ed6-ff77ceb08bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the categorical cross-entropy loss function\n",
    "def categorical_cross_entropy(target, output):\n",
    "    return -sum([target[i] * output[i].log() for i in range(len(target))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbf45bea-1fdd-40fd-b1be-7a6ead0b2b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def loss(batch_size=None):\n",
    "    if batch_size is None:\n",
    "        Xb, yb = X_train, y_train\n",
    "    else:\n",
    "        ri = np.random.permutation(X_train.shape[0])[:batch_size]\n",
    "        Xb, yb = X_train[ri], y_train[ri]\n",
    "\n",
    "    inputs = [list(map(Value, xrow)) for xrow in Xb]\n",
    "    scores = [model.forward(x) for x in inputs]\n",
    "\n",
    "    #yb is a batch of true labels (one-hot encoded)\n",
    "    losses = [categorical_cross_entropy(yb[i], scores[i]) for i in range(len(yb))]\n",
    "    \n",
    "    data_loss = sum(losses) * (1.0 / len(losses))\n",
    "\n",
    "    alpha = 1e-4\n",
    "    reg_loss = alpha * sum((p * p for p in model.parameters()))\n",
    "    total_loss = data_loss + reg_loss\n",
    "\n",
    "    accuracy = [np.argmax(yb[i]) == np.argmax([s.data for s in scores[i]]) for i in range(len(yb))]\n",
    "    return total_loss, sum(accuracy) / len(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cee8b4c-3aa3-4e1a-b6d8-e0cfb5a822f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate initial loss and accuracy\n",
    "total_loss, acc = loss()\n",
    "print(\"Total loss:\", total_loss.data, \", Accuracy:\", acc * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12a8809-b755-4212-91d0-4d6e61b063dc",
   "metadata": {},
   "source": [
    "### Training Loop with Backward Pass and Parameter Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbc235b-ed58-4002-be57-c344575d977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization (training loop)\n",
    "for k in range(100):\n",
    "    total_loss, acc = loss(batch_size=32)\n",
    "    model.zero_grad()\n",
    "    total_loss.backward()\n",
    "    learning_rate = 0.01 - 0.0001 * k\n",
    "    for p in model.parameters():\n",
    "        p.data -= learning_rate * p.grad\n",
    "    \n",
    "    if k % 10 == 0:\n",
    "        print(f\"Step {k + 1} loss {total_loss.data}, accuracy {acc * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c8d3b6-780c-416d-905b-2c4e7daebfb3",
   "metadata": {},
   "source": [
    "### Evaluation on Test Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ce7cde-7fdb-4298-92ad-4a875fb288da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "def evaluate():\n",
    "    inputs = [list(map(Value, xrow)) for xrow in X_test]\n",
    "    scores = [model.forward(x) for x in inputs]\n",
    "    accuracy = [np.argmax(y_test[i]) == np.argmax([s.data for s in scores[i]]) for i in range(len(y_test))]\n",
    "    return sum(accuracy) / len(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe00aa6-4c87-4b78-9e59-c486c8cac821",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = evaluate()\n",
    "print(f\"Test accuracy: {test_acc * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc311c6a-cdaf-401a-b496-0d2f50dcdf40",
   "metadata": {},
   "source": [
    "### Visualizing Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5578d812-08b5-4902-abe1-b111d749ebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few test images along with their predicted and true labels\n",
    "fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "fig.suptitle('CIFAR-10 Predictions')\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = X_test[i].reshape(32, 32, 3)\n",
    "    true_label = np.argmax(y_test[i])\n",
    "    pred_label = np.argmax([s.data for s in model.forward(list(map(Value, X_test[i])))])\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f'True: {true_label}, Pred: {pred_label}')\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c00635d-9cd1-4383-851c-750d213d37bc",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "This complete code leverages the micrograd library to build and train an MLP model for CIFAR-10 image classification, including data preprocessing, model definition, training, evaluation, and visualization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
